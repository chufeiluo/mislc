{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea29361-eee1-4ca4-9984-e41f714939fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d835032e-4d60-457d-b0f1-53efd0abd211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/14cfl/anaconda3/envs/torch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-06-03 17:14:33,304\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 17:14:34 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Meta-Llama-3-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 17:14:38 utils.py:660] Found nccl from library /home/14cfl/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:38 utils.py:660] Found nccl from library /home/14cfl/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-03 17:14:39 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 06-03 17:14:39 selector.py:32] Using XFormers backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:39 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:39 selector.py:32] Using XFormers backend.\n",
      "INFO 06-03 17:14:40 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:40 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 06-03 17:14:41 utils.py:132] reading GPU P2P access cache from /home/14cfl/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 06-03 17:14:41 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:41 utils.py:132] reading GPU P2P access cache from /home/14cfl/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m WARNING 06-03 17:14:41 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 06-03 17:14:42 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:42 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-03 17:14:44 model_runner.py:175] Loading model weights took 7.4829 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:44 model_runner.py:175] Loading model weights took 7.4829 GB\n",
      "INFO 06-03 17:14:49 distributed_gpu_executor.py:45] # GPU blocks: 12353, # CPU blocks: 4096\n",
      "INFO 06-03 17:14:50 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-03 17:14:50 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:50 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:50 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=359086)\u001b[0m INFO 06-03 17:14:57 model_runner.py:1017] Graph capturing finished in 7 secs.\n",
      "INFO 06-03 17:14:57 model_runner.py:1017] Graph capturing finished in 7 secs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm = LLM(model='meta-llama/Meta-Llama-3-8B', tensor_parallel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2befcb-fa19-4914-b40c-7c4a1069071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=1, logprobs=1)\n",
    "\n",
    "out = llm.generate(\n",
    "    'hello ',\n",
    "    sampling_params=sampling_params,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2459136-f92c-453c-a5f8-6a8d6226477a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RequestOutput(request_id=0, prompt='hello ', prompt_token_ids=[128000, 15339, 220], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='1 1 1 1 1 1 1 1 ', token_ids=[16, 220, 16, 220, 16, 220, 16, 220, 16, 220, 16, 220, 16, 220, 16, 220], cumulative_logprob=-7.239102125633508, logprobs=[{16: Logprob(logprob=-2.270709991455078, rank=4, decoded_token='1'), 679: Logprob(logprob=-1.9582099914550781, rank=1, decoded_token='201')}, {220: Logprob(logprob=-1.5283360481262207, rank=1, decoded_token=' ')}, {16: Logprob(logprob=-2.084290027618408, rank=2, decoded_token='1'), 17: Logprob(logprob=-1.8342901468276978, rank=1, decoded_token='2')}, {220: Logprob(logprob=-0.46572256088256836, rank=1, decoded_token=' ')}, {16: Logprob(logprob=-0.4829452335834503, rank=1, decoded_token='1')}, {220: Logprob(logprob=-0.06125149503350258, rank=1, decoded_token=' ')}, {16: Logprob(logprob=-0.07915917783975601, rank=1, decoded_token='1')}, {220: Logprob(logprob=-0.053047262132167816, rank=1, decoded_token=' ')}, {16: Logprob(logprob=-0.036334287375211716, rank=1, decoded_token='1')}, {220: Logprob(logprob=-0.06461780518293381, rank=1, decoded_token=' ')}, {16: Logprob(logprob=-0.013878156431019306, rank=1, decoded_token='1')}, {220: Logprob(logprob=-0.03172684088349342, rank=1, decoded_token=' ')}, {16: Logprob(logprob=-0.008395852521061897, rank=1, decoded_token='1')}, {220: Logprob(logprob=-0.026747506111860275, rank=1, decoded_token=' ')}, {16: Logprob(logprob=-0.007756590377539396, rank=1, decoded_token='1')}, {220: Logprob(logprob=-0.02418329007923603, rank=1, decoded_token=' ')}], finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1717449388.6082516, last_token_time=1717449388.6082516, first_scheduled_time=1717449388.612187, first_token_time=1717449388.6748133, time_in_queue=0.003935337066650391, finished_time=1717449388.9071457), lora_request=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2278c4a-f718-4859-a03f-264c20702d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 1 1 1 1 1 1 1 '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7dee592-a11e-4943-9e76-168d56e939a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 220, 16, 220, 16, 220, 16, 220, 16, 220, 16, 220, 16, 220, 16, 220]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].outputs[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e23761f0-71aa-4c1c-bb0d-8466fe5777b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{16: Logprob(logprob=-2.270709991455078, rank=4, decoded_token='1'),\n",
       "  679: Logprob(logprob=-1.9582099914550781, rank=1, decoded_token='201')},\n",
       " {220: Logprob(logprob=-1.5283360481262207, rank=1, decoded_token=' ')},\n",
       " {16: Logprob(logprob=-2.084290027618408, rank=2, decoded_token='1'),\n",
       "  17: Logprob(logprob=-1.8342901468276978, rank=1, decoded_token='2')},\n",
       " {220: Logprob(logprob=-0.46572256088256836, rank=1, decoded_token=' ')},\n",
       " {16: Logprob(logprob=-0.4829452335834503, rank=1, decoded_token='1')},\n",
       " {220: Logprob(logprob=-0.06125149503350258, rank=1, decoded_token=' ')},\n",
       " {16: Logprob(logprob=-0.07915917783975601, rank=1, decoded_token='1')},\n",
       " {220: Logprob(logprob=-0.053047262132167816, rank=1, decoded_token=' ')},\n",
       " {16: Logprob(logprob=-0.036334287375211716, rank=1, decoded_token='1')},\n",
       " {220: Logprob(logprob=-0.06461780518293381, rank=1, decoded_token=' ')},\n",
       " {16: Logprob(logprob=-0.013878156431019306, rank=1, decoded_token='1')},\n",
       " {220: Logprob(logprob=-0.03172684088349342, rank=1, decoded_token=' ')},\n",
       " {16: Logprob(logprob=-0.008395852521061897, rank=1, decoded_token='1')},\n",
       " {220: Logprob(logprob=-0.026747506111860275, rank=1, decoded_token=' ')},\n",
       " {16: Logprob(logprob=-0.007756590377539396, rank=1, decoded_token='1')},\n",
       " {220: Logprob(logprob=-0.02418329007923603, rank=1, decoded_token=' ')}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].outputs[0].logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90e0ae5a-82fa-48d6-b8f7-2c31f08cc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = []\n",
    "\n",
    "for s in out[0].outputs:\n",
    "    tokens = s.token_ids\n",
    "    beams = s.logprobs\n",
    "    temp = []\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in beams[i]: # each beam is a dictionary of tokens\n",
    "            temp.append(beams[i][tokens[i]].logprob)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "            temp.append(1)\n",
    "    logprobs.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91656789-f1c0-4462-a07b-dc51054c16be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.270709991455078,\n",
       "  -1.5283360481262207,\n",
       "  -2.084290027618408,\n",
       "  -0.46572256088256836,\n",
       "  -0.4829452335834503,\n",
       "  -0.06125149503350258,\n",
       "  -0.07915917783975601,\n",
       "  -0.053047262132167816,\n",
       "  -0.036334287375211716,\n",
       "  -0.06461780518293381,\n",
       "  -0.013878156431019306,\n",
       "  -0.03172684088349342,\n",
       "  -0.008395852521061897,\n",
       "  -0.026747506111860275,\n",
       "  -0.007756590377539396,\n",
       "  -0.02418329007923603]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1422b9-459f-4e39-b212-487705b4fe66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
